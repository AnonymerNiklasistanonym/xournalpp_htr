{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment w clustering for online word detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from xournalpp_htr.documents import XournalppDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../tests/data/2024-07-26_minimal.xopp\"  # Acutal baseline\n",
    "# PATH = \"/home/martin/data/xournalpp_htr/test.xopp\"  # Exciting experiment\n",
    "\n",
    "xpp_doc = XournalppDocument(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_PAGE = 0\n",
    "DPI = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot example data as-is w/o segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = xpp_doc.pages[I_PAGE]\n",
    "\n",
    "all_strokes = []\n",
    "\n",
    "for layer in page.layers:\n",
    "    for stroke in layer.strokes:\n",
    "        x = stroke.x / DPI\n",
    "        y = stroke.y / DPI\n",
    "\n",
    "        y *= -1\n",
    "\n",
    "        all_strokes.append({\"x\": x, \"y\": y, \"x_mean\": x.mean(), \"y_mean\": y.mean()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\n",
    "    figsize=(\n",
    "        float(page.meta_data[\"width\"]) / DPI,\n",
    "        float(page.meta_data[\"height\"]) / DPI,\n",
    "    )\n",
    ")\n",
    "\n",
    "for stroke in all_strokes:\n",
    "    x = stroke[\"x\"]\n",
    "    y = stroke[\"y\"]\n",
    "    plt.scatter(x, y, s=1, c=\"black\")\n",
    "\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1\n",
    "\n",
    "Plot mean of each stroke on top of stroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\n",
    "    figsize=(\n",
    "        float(page.meta_data[\"width\"]) / DPI,\n",
    "        float(page.meta_data[\"height\"]) / DPI,\n",
    "    )\n",
    ")\n",
    "\n",
    "for stroke in all_strokes:\n",
    "    plt.scatter(stroke[\"x\"], stroke[\"y\"], s=1, c=\"black\")\n",
    "    plt.scatter(stroke[\"x_mean\"], stroke[\"y_mean\"], c=\"red\", s=1)\n",
    "\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I compute the distance between the strokes and threshold it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.infty * np.ones((len(all_strokes), len(all_strokes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_strokes)):\n",
    "    for j in range(i - 1 + 1):\n",
    "        element = (all_strokes[i][\"x_mean\"] - all_strokes[j][\"x_mean\"]) ** 2 + (\n",
    "            all_strokes[i][\"y_mean\"] - all_strokes[j][\"y_mean\"]\n",
    "        ) ** 2\n",
    "        distances[i, j] = element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[distances != -1.0].min(), distances[distances != -1.0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the clusters (inefficiently for now just to get it running at first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_groups = []\n",
    "\n",
    "for i in range(1, len(all_strokes)):\n",
    "    # Check if I want to skip `i` b/c it's already part of a stroke group\n",
    "    skip_i = False\n",
    "    for stroke_group in stroke_groups:\n",
    "        if i in stroke_group:\n",
    "            skip_i = True\n",
    "            break\n",
    "    if skip_i:\n",
    "        continue\n",
    "\n",
    "    # Construct a stroke group\n",
    "    similar_strokes = np.where(distances[i] <= THRESHOLD)[0]\n",
    "\n",
    "    # print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, i didn't manage to implement it naively b/c I wasn't concentrated. instead, I will use clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"i need to implement that!\")\n",
    "\n",
    "already_used = np.zeros(len(all_strokes), dtype=bool)\n",
    "\n",
    "stroke_groups = []\n",
    "\n",
    "for i in range(1, len(all_strokes)):\n",
    "    if not already_used[i]:\n",
    "        word_members = []\n",
    "        similar_strokes = np.where(distances[i] <= THRESHOLD)[0]\n",
    "        similar_strokes = similar_strokes.tolist() + [\n",
    "            i,\n",
    "        ]\n",
    "\n",
    "        print(similar_strokes)\n",
    "\n",
    "        stroke_groups.append(similar_strokes)\n",
    "\n",
    "        for j in similar_strokes:\n",
    "            already_used[j] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read: [1](https://stats.stackexchange.com/questions/148161/clustering-from-similarity-distance-matrix), [2](https://scikit-learn.org/dev/modules/generated/sklearn.cluster.SpectralClustering.html), [3](https://stats.stackexchange.com/questions/475687/clustering-given-distance-matrix-and-k-in-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of this approach is that the thresholding is not scale invariant, i.e. larger written words will fail to be recognised to be a word b/c the strokes are too far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2\n",
    "\n",
    "B/c/ I didn't manage to get the naive approach running (lack of concentration and problem understanding), I decided to try out clustering directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\n",
    "    figsize=(\n",
    "        float(page.meta_data[\"width\"]) / DPI,\n",
    "        float(page.meta_data[\"height\"]) / DPI,\n",
    "    )\n",
    ")\n",
    "\n",
    "for stroke in all_strokes:\n",
    "    plt.scatter(stroke[\"x\"], stroke[\"y\"], s=1, c=\"black\")\n",
    "    plt.scatter(stroke[\"x_mean\"], stroke[\"y_mean\"], c=\"red\", s=1)\n",
    "\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, assemble the `X` variable with covariates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for stroke in all_strokes:\n",
    "    X.append([stroke[\"x_mean\"], stroke[\"y_mean\"]])\n",
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try out [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) w hard-coded number of clusters to check if it then finds the right words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "N_CLUSTERS = 22\n",
    "DISTANCE_THRESHOLD = None\n",
    "\n",
    "N_CLUSTERS = 10\n",
    "DISTANCE_THRESHOLD = None\n",
    "\n",
    "N_CLUSTERS = None\n",
    "DISTANCE_THRESHOLD = 1e0  # One could maybe tune it by investigating nr of clusters over distance threshold\n",
    "\n",
    "clustering = AgglomerativeClustering(\n",
    "    n_clusters=N_CLUSTERS, distance_threshold=DISTANCE_THRESHOLD\n",
    ").fit(X)  # I hard-code 22 b/c I counted that there're 22 clusters\n",
    "np.unique(clustering.labels_), np.unique(clustering.labels_).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check if the clusters make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\n",
    "    figsize=(\n",
    "        float(page.meta_data[\"width\"]) / DPI,\n",
    "        float(page.meta_data[\"height\"]) / DPI,\n",
    "    )\n",
    ")\n",
    "\n",
    "for i_cluster in np.unique(clustering.labels_):\n",
    "    stroke_indices = np.where(clustering.labels_ == i_cluster)[0]\n",
    "\n",
    "    print(i_cluster, stroke_indices)\n",
    "\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    x_coords_mean = []\n",
    "    y_coords_mean = []\n",
    "    for stroke_index in stroke_indices:\n",
    "        stroke = all_strokes[stroke_index]\n",
    "        x_coords += stroke[\"x\"].tolist()\n",
    "        y_coords += stroke[\"y\"].tolist()\n",
    "        x_coords_mean.append(stroke[\"x_mean\"])\n",
    "        y_coords_mean.append(stroke[\"y_mean\"])\n",
    "\n",
    "    plt.scatter(x_coords, y_coords, s=1)\n",
    "    plt.scatter(x_coords_mean, y_coords_mean, c=\"red\", s=1)\n",
    "\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the result is not too bad!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, investigate number of clusters against threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = None\n",
    "\n",
    "distance_thresholds = []\n",
    "number_of_clusters = []\n",
    "\n",
    "for DISTANCE_THRESHOLD in np.logspace(-4, 1, 1000):\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=N_CLUSTERS, distance_threshold=DISTANCE_THRESHOLD\n",
    "    ).fit(X)\n",
    "\n",
    "    distance_thresholds.append(DISTANCE_THRESHOLD)\n",
    "    number_of_clusters.append(np.unique(clustering.labels_).shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(distance_thresholds, number_of_clusters)\n",
    "plt.xlabel(\"distance_thresholds\")\n",
    "plt.ylabel(\"number_of_clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^-- Unfortunately, one cannot see a clear sign of what distance threshold to pick. This is probably a function of the the content of the page (e.g. diagrams, written text height, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, question: Is this approach robust against larger handwriting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the dendrogram, see [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try out DBSCAN! Also see [here](https://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also next, try out another document to play around with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Is my OnlineHTR model robust against rotated text?! Maybe one should rotate the text first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is probably worth it to write a bit of infrastructure code to experiment more (and easier and easier to compare) with these clustering approaches.\n",
    "\n",
    "Next: Feed these sequences to `OnlineHTR` or retrained `SimpleHTR` nmodel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attemp 3: DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "eps_values = []\n",
    "number_of_clusters = []\n",
    "\n",
    "for EPS in np.logspace(-4, 1, 1000):\n",
    "    clustering = DBSCAN(\n",
    "        eps=EPS,\n",
    "    ).fit(X)\n",
    "\n",
    "    eps_values.append(EPS)\n",
    "    number_of_clusters.append(np.unique(clustering.labels_).shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(eps_values, number_of_clusters)\n",
    "plt.xlabel(\"eps\")\n",
    "plt.ylabel(\"number_of_clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is a bit unconclusive for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_max = eps_values[np.argmax(number_of_clusters)]\n",
    "print(eps_max)\n",
    "\n",
    "clustering = DBSCAN(\n",
    "    eps=eps_max,\n",
    ").fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\n",
    "    figsize=(\n",
    "        float(page.meta_data[\"width\"]) / DPI,\n",
    "        float(page.meta_data[\"height\"]) / DPI,\n",
    "    )\n",
    ")\n",
    "\n",
    "for i_cluster in np.unique(clustering.labels_):\n",
    "    stroke_indices = np.where(clustering.labels_ == i_cluster)[0]\n",
    "\n",
    "    print(i_cluster, stroke_indices)\n",
    "\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    x_coords_mean = []\n",
    "    y_coords_mean = []\n",
    "    for stroke_index in stroke_indices:\n",
    "        stroke = all_strokes[stroke_index]\n",
    "        x_coords += stroke[\"x\"].tolist()\n",
    "        y_coords += stroke[\"y\"].tolist()\n",
    "        x_coords_mean.append(stroke[\"x_mean\"])\n",
    "        y_coords_mean.append(stroke[\"y_mean\"])\n",
    "\n",
    "    if i_cluster == -1:\n",
    "        plt.scatter(x_coords, y_coords, s=1, alpha=1, c=\"fuchsia\")\n",
    "    else:\n",
    "        plt.scatter(x_coords, y_coords, s=1)\n",
    "    plt.scatter(x_coords_mean, y_coords_mean, c=\"red\", s=1)\n",
    "\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DBSCAN method seems to throw many of the words in the noisy class; this is probably worth investigating. Other than that, it seems to work OK-ish.\n",
    "\n",
    "Interestingly, I think the biggest problem for the OnlineHTR model would be the different line positions based on the way it was trained. Hence, one could maybe put extra emphasis on clusters being on similar y values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT: Spectral clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to say that I am unclear if a heuristic (i.e. a clustering algo w/ smartly chosen parameters) is really enough. Certainly for now, but a fully data-driven way would be better to accommodate different writers. This is probably relevant for a next iteration of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xournalpp_htr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
