{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment w clustering for online word detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from xournalpp_htr.training.io import load_list_of_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment structure\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "One can find an algorithm that segments strokes into words using my handwriting.\n",
    "\n",
    "Side note: This is useful b/c I can then use [OnlineHTR](https://github.com/PellelNitram/OnlineHTR) to transcribe the words.\n",
    "\n",
    "### Notebook structure\n",
    "\n",
    "1. Load data, incl ground truth.\n",
    "2. Pre-compute a set of features. Later, feature engineering might be added.\n",
    "3. Iterate over a few algorithms and measure their performance using the ground truth.\n",
    "\n",
    "Alternative addition later on: Manually remove strokes that're too long (in distribution sense) or too straight. That is another step because it will require a dataset with such strokes that don't belong to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "\n",
    "OUTPUT_PATH = Path(\"experiment_results\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "def plot_clustered_document(\n",
    "    a_ground_truth: mpl.axis.Axis,\n",
    "    a_predicted: mpl.axis.Axis,\n",
    "    clustering,\n",
    "    annotated_bboxes,\n",
    "    DPI,\n",
    "    df_train: pd.DataFrame,\n",
    "    a_predicted_title: str,\n",
    ") -> None:\n",
    "    \"\"\"Plots clustered document into axes.\"\"\"\n",
    "\n",
    "    # ===================\n",
    "    # Ground truth figure\n",
    "    # ===================\n",
    "\n",
    "    # I replicated this from below - TODO: Follow DIY and consolidate\n",
    "\n",
    "    a_ground_truth.set_aspect(\"equal\")\n",
    "    a_ground_truth.set_xlabel(\"x\")\n",
    "    a_ground_truth.set_ylabel(\"y\")\n",
    "\n",
    "    for i_bbox in range(len(annotated_bboxes[\"bboxes\"])):\n",
    "        bbox = annotated_bboxes[\"bboxes\"][i_bbox]\n",
    "\n",
    "        # Draw bbox\n",
    "        xy = (\n",
    "            min([bbox[\"point_1_x\"], bbox[\"point_2_x\"]]) / DPI,\n",
    "            min([-bbox[\"point_1_y\"], -bbox[\"point_2_y\"]])\n",
    "            / DPI,  # TODO: This messing around w/ y coord sign is annoying\n",
    "        )\n",
    "        dx = np.abs(bbox[\"point_1_x\"] - bbox[\"point_2_x\"]) / DPI\n",
    "        dy = np.abs(bbox[\"point_1_y\"] - bbox[\"point_2_y\"]) / DPI\n",
    "        a_ground_truth.add_patch(\n",
    "            patches.Rectangle(xy, dx, dy, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "        )\n",
    "\n",
    "        # Draw label\n",
    "        a_ground_truth.text(x=xy[0], y=xy[1] + dy, s=bbox[\"text\"], c=\"red\")\n",
    "\n",
    "        for bbox_stroke in bbox[\"bbox_strokes\"]:\n",
    "            x = bbox_stroke[\"x\"] / DPI\n",
    "            y = bbox_stroke[\"y\"] / DPI\n",
    "            a_ground_truth.scatter(x, -y, c=\"black\", s=1)\n",
    "\n",
    "    # ================\n",
    "    # Predicted figure\n",
    "    # ================\n",
    "\n",
    "    a_predicted.set_aspect(\"equal\")\n",
    "    a_predicted.set_xlabel(\"x\")\n",
    "    a_predicted.set_ylabel(\"y\")\n",
    "    a_predicted.set_title(a_predicted_title)\n",
    "\n",
    "    for i_cluster in np.unique(clustering.labels_):\n",
    "        stroke_indices = np.where(clustering.labels_ == i_cluster)[0]\n",
    "\n",
    "        print(i_cluster, stroke_indices)\n",
    "\n",
    "        x_coords = []\n",
    "        y_coords = []\n",
    "        x_coords_mean = []\n",
    "        y_coords_mean = []\n",
    "        for stroke_index in stroke_indices:\n",
    "            stroke_row = df_train.iloc[stroke_index]\n",
    "\n",
    "            x_coords += stroke_row[\"x\"].tolist()\n",
    "            y_coords += stroke_row[\"y\"].tolist()\n",
    "            x_coords_mean.append(stroke_row[\"x_mean\"])\n",
    "            y_coords_mean.append(stroke_row[\"y_mean\"])\n",
    "\n",
    "        a_predicted.scatter(x_coords, y_coords, s=1)\n",
    "        a_predicted.scatter(x_coords_mean, y_coords_mean, c=\"red\", s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, I loaded the data as `XournalppDocument` but that approach lacked ground truth data. Instead, I now load the annotated data, which comes with ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_bboxes = load_list_of_bboxes(\n",
    "    \"../tests/data/2024-10-13_minimal.annotations.json\"\n",
    ")\n",
    "\n",
    "DPI = 72  # TODO: Add this to annotations!\n",
    "\n",
    "# TODO: Maybe integrate `/DPI` into the x and y values? Maybe convert to cm?\n",
    "# TODO: Add page dimensions, i.e.:\n",
    "# - float(page.meta_data[\"width\"]) / DPI,\n",
    "# - float(page.meta_data[\"height\"]) / DPI,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========\n",
    "# Figure 1\n",
    "# ========\n",
    "\n",
    "length = len(annotated_bboxes[\"bboxes\"])\n",
    "nr_2 = 4\n",
    "nr_1 = length // nr_2 + 1\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nr_1, ncols=nr_2, figsize=(10, 8))\n",
    "\n",
    "for i_bbox in range(length):\n",
    "    bbox = annotated_bboxes[\"bboxes\"][i_bbox]\n",
    "\n",
    "    a = axes.flatten()[i_bbox]\n",
    "\n",
    "    a.set_aspect(\"equal\")\n",
    "    a.set_title(bbox[\"text\"])\n",
    "    a.set_xlabel(\"x\")\n",
    "    a.set_ylabel(\"-y\")\n",
    "\n",
    "    for bbox_stroke in bbox[\"bbox_strokes\"]:\n",
    "        x = bbox_stroke[\"x\"] / DPI\n",
    "        y = bbox_stroke[\"y\"] / DPI\n",
    "        a.scatter(x, -y, c=\"black\", s=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========\n",
    "# Figure 2\n",
    "# ========\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "a = plt.gca()\n",
    "a.set_aspect(\"equal\")\n",
    "a.set_xlabel(\"x\")\n",
    "a.set_ylabel(\"-y\")\n",
    "\n",
    "for i_bbox in range(length):\n",
    "    bbox = annotated_bboxes[\"bboxes\"][i_bbox]\n",
    "\n",
    "    # Draw bbox\n",
    "    xy = (\n",
    "        min([bbox[\"point_1_x\"], bbox[\"point_2_x\"]]) / DPI,\n",
    "        min([-bbox[\"point_1_y\"], -bbox[\"point_2_y\"]])\n",
    "        / DPI,  # TODO: This messing around w/ y coord sign is annoying\n",
    "    )\n",
    "    dx = np.abs(bbox[\"point_1_x\"] - bbox[\"point_2_x\"]) / DPI\n",
    "    dy = np.abs(bbox[\"point_1_y\"] - bbox[\"point_2_y\"]) / DPI\n",
    "    a.add_patch(\n",
    "        patches.Rectangle(xy, dx, dy, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "    )\n",
    "\n",
    "    # Draw label\n",
    "    a.text(x=xy[0], y=xy[1] + dy, s=bbox[\"text\"], c=\"red\")\n",
    "\n",
    "    for bbox_stroke in bbox[\"bbox_strokes\"]:\n",
    "        x = bbox_stroke[\"x\"] / DPI\n",
    "        y = bbox_stroke[\"y\"] / DPI\n",
    "        a.scatter(x, -y, c=\"black\", s=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare list of all strokes w/ relevant meta information as ground truth. This variable serves as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strokes_data = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "    \"x_mean\": [],\n",
    "    \"y_mean\": [],\n",
    "    \"i_bbox\": [],\n",
    "    \"text\": [],\n",
    "}\n",
    "\n",
    "for i_bbox in range(len(annotated_bboxes[\"bboxes\"])):\n",
    "    bbox = annotated_bboxes[\"bboxes\"][i_bbox]\n",
    "\n",
    "    for bbox_stroke in bbox[\"bbox_strokes\"]:\n",
    "        x = +bbox_stroke[\"x\"] / DPI\n",
    "        y = -bbox_stroke[\"y\"] / DPI\n",
    "\n",
    "        df_strokes_data[\"x\"].append(x)\n",
    "        df_strokes_data[\"y\"].append(y)\n",
    "        df_strokes_data[\"x_mean\"].append(np.mean(x))\n",
    "        df_strokes_data[\"y_mean\"].append(np.mean(y))\n",
    "        df_strokes_data[\"i_bbox\"].append(i_bbox)\n",
    "        df_strokes_data[\"text\"].append(bbox[\"text\"])\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(df_strokes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "a = plt.gca()\n",
    "a.set_aspect(\"equal\")\n",
    "a.set_xlabel(\"x\")\n",
    "a.set_ylabel(\"y\")\n",
    "\n",
    "for (i_bbox, text), df_grouped in df_train.groupby(\n",
    "    [\"i_bbox\", \"text\"],\n",
    "):\n",
    "    a.scatter(df_grouped[\"x_mean\"], df_grouped[\"y_mean\"], c=\"red\", s=2, zorder=999)\n",
    "\n",
    "    bottom_left_x = np.inf\n",
    "    bottom_left_y = np.inf\n",
    "    top_right_x = -np.inf\n",
    "    top_right_y = -np.inf\n",
    "    for _, row in df_grouped.iterrows():\n",
    "        a.plot(row.x, row.y)  # , c=cmap(i_row/N))\n",
    "        if row.x.min() < bottom_left_x:\n",
    "            bottom_left_x = row.x.min()\n",
    "        if row.y.min() < bottom_left_y:\n",
    "            bottom_left_y = row.y.min()\n",
    "        if row.x.max() > top_right_x:\n",
    "            top_right_x = row.x.max()\n",
    "        if row.y.max() > top_right_y:\n",
    "            top_right_y = row.y.max()\n",
    "\n",
    "    # Plot bounding box\n",
    "    xy = (bottom_left_x, bottom_left_y)\n",
    "    dx = top_right_x - bottom_left_x\n",
    "    dy = top_right_y - bottom_left_y\n",
    "    a.add_patch(\n",
    "        patches.Rectangle(xy, dx, dy, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "    )\n",
    "\n",
    "    # Plot text\n",
    "    a.text(x=bottom_left_x, y=top_right_y, s=f'\"{text}\" ({i_bbox})', c=\"red\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_clusterings = [\n",
    "    AgglomerativeClustering(\n",
    "        n_clusters=22, distance_threshold=None\n",
    "    ),  # I hard-code 22 b/c I counted that there're 22 clusters\n",
    "    AgglomerativeClustering(n_clusters=10, distance_threshold=None),\n",
    "    AgglomerativeClustering(\n",
    "        n_clusters=None, distance_threshold=1e0\n",
    "    ),  # One could maybe tune it by investigating nr of clusters over distance threshold; TODO: Distance threshold using distribution?!\n",
    "    SpectralClustering(\n",
    "        n_clusters=15,  # 21,\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    ),\n",
    "    SpectralClustering(\n",
    "        n_clusters=21,  # 21,\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    ),\n",
    "    SpectralClustering(\n",
    "        n_clusters=6,  # 21,\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "all_clusterings += [DBSCAN(eps) for eps in np.logspace(-4, 1, 1000)]\n",
    "\n",
    "results = {\n",
    "    \"index\": [],\n",
    "    \"score\": [],\n",
    "}\n",
    "for i_clustering, clustering in enumerate(all_clusterings):\n",
    "    clustering.fit(df_train[[\"x_mean\", \"y_mean\"]])\n",
    "\n",
    "    score = adjusted_rand_score(df_train[\"i_bbox\"], clustering.labels_)\n",
    "\n",
    "    results[\"index\"].append(i_clustering)\n",
    "    results[\"score\"].append(score)\n",
    "\n",
    "    # Plotting\n",
    "    fig, [a_ground_truth, a_predicted] = plt.subplots(1, 2, figsize=(10, 8))\n",
    "    plot_clustered_document(\n",
    "        a_ground_truth,\n",
    "        a_predicted,\n",
    "        clustering,\n",
    "        annotated_bboxes,\n",
    "        DPI,\n",
    "        df_train,\n",
    "        a_predicted_title=f\"A-RAND={score}\",\n",
    "    )\n",
    "    plt.savefig(OUTPUT_PATH / f\"iClustering{i_clustering}.png\")\n",
    "    plt.close()\n",
    "\n",
    "results = pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(results[\"index\"], results[\"score\"], c=\"red\")\n",
    "\n",
    "plt.xlabel(\"Index of clustering settings\")\n",
    "plt.ylabel(\"Adjusted Rand Score (larger is better)\")\n",
    "plt.savefig(\"2024-10-18_clustering_experiments.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: The peak at ~800 seems to classify rows of text. This should be fine w/ OnlineHTR!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check if the clusters make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the result is not too bad!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, investigate number of clusters against threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_CLUSTERS = None\n",
    "\n",
    "# distance_thresholds = []\n",
    "# number_of_clusters = []\n",
    "\n",
    "# for DISTANCE_THRESHOLD in np.logspace(-4, 1, 1000):\n",
    "#     clustering = AgglomerativeClustering(\n",
    "#         n_clusters=N_CLUSTERS, distance_threshold=DISTANCE_THRESHOLD\n",
    "#     ).fit(X)\n",
    "\n",
    "#     distance_thresholds.append(DISTANCE_THRESHOLD)\n",
    "#     number_of_clusters.append(np.unique(clustering.labels_).shape)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(distance_thresholds, number_of_clusters)\n",
    "# plt.xlabel(\"distance_thresholds\")\n",
    "# plt.ylabel(\"number_of_clusters\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^-- Unfortunately, one cannot see a clear sign of what distance threshold to pick. This is probably a function of the the content of the page (e.g. diagrams, written text height, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, question: Is this approach robust against larger handwriting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the dendrogram, see [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try out DBSCAN! Also see [here](https://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also next, try out another document to play around with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Is my OnlineHTR model robust against rotated text?! Maybe one should rotate the text first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is probably worth it to write a bit of infrastructure code to experiment more (and easier and easier to compare) with these clustering approaches.\n",
    "\n",
    "Next: Feed these sequences to `OnlineHTR` or retrained `SimpleHTR` nmodel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attemp 3: DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(\n",
    "#     figsize=(\n",
    "#         float(page.meta_data[\"width\"]) / DPI,\n",
    "#         float(page.meta_data[\"height\"]) / DPI,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# for i_cluster in np.unique(clustering.labels_):\n",
    "#     stroke_indices = np.where(clustering.labels_ == i_cluster)[0]\n",
    "\n",
    "#     print(i_cluster, stroke_indices)\n",
    "\n",
    "#     x_coords = []\n",
    "#     y_coords = []\n",
    "#     x_coords_mean = []\n",
    "#     y_coords_mean = []\n",
    "#     for stroke_index in stroke_indices:\n",
    "#         stroke = all_strokes[stroke_index]\n",
    "#         x_coords += stroke[\"x\"].tolist()\n",
    "#         y_coords += stroke[\"y\"].tolist()\n",
    "#         x_coords_mean.append(stroke[\"x_mean\"])\n",
    "#         y_coords_mean.append(stroke[\"y_mean\"])\n",
    "\n",
    "#     if i_cluster == -1:\n",
    "#         plt.scatter(x_coords, y_coords, s=1, alpha=1, c=\"fuchsia\")\n",
    "#     else:\n",
    "#         plt.scatter(x_coords, y_coords, s=1)\n",
    "#     plt.scatter(x_coords_mean, y_coords_mean, c=\"red\", s=1)\n",
    "\n",
    "# plt.gca().set_aspect(\"equal\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DBSCAN method seems to throw many of the words in the noisy class; this is probably worth investigating. Other than that, it seems to work OK-ish.\n",
    "\n",
    "Interestingly, I think the biggest problem for the OnlineHTR model would be the different line positions based on the way it was trained. Hence, one could maybe put extra emphasis on clusters being on similar y values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT: Spectral clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to say that I am unclear if a heuristic (i.e. a clustering algo w/ smartly chosen parameters) is really enough. Certainly for now, but a fully data-driven way would be better to accommodate different writers. This is probably relevant for a next iteration of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "- Good source for rand score: [see here](https://stats.stackexchange.com/questions/260229/comparing-a-clustering-algorithm-partition-to-a-ground-truth-one).\n",
    "- After finding the best clustering, do apply OnlineHTR to check how it performs!\n",
    "- To overcome the scale issue (i.e. everyone's handwriting scale is a wee bit different), one would need to use an approach that is based on 'nearest neighbours'. This works b/c one does not write on top of existing words.\n",
    "    - also, one could weight the x direction more in definition of closeness/distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xournalpp_htr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
