{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment w clustering for online word detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import (\n",
    "    DBSCAN,\n",
    "    HDBSCAN,\n",
    "    AffinityPropagation,\n",
    "    AgglomerativeClustering,\n",
    "    MeanShift,\n",
    "    SpectralClustering,\n",
    ")\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from xournalpp_htr.training.io import load_list_of_bboxes\n",
    "from xournalpp_htr.training.visualise import plot_clustered_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment structure\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "One can find an algorithm that segments strokes into words using my handwriting.\n",
    "\n",
    "Side note: This is useful b/c I can then use [OnlineHTR](https://github.com/PellelNitram/OnlineHTR) to transcribe the words.\n",
    "\n",
    "### Notebook structure\n",
    "\n",
    "1. Load data, incl ground truth.\n",
    "2. Pre-compute a set of features. Later, feature engineering might be added.\n",
    "3. Iterate over a few algorithms and measure their performance using the ground truth.\n",
    "\n",
    "Alternative addition later on: Manually remove strokes that're too long (in distribution sense) or too straight. That is another step because it will require a dataset with such strokes that don't belong to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "\n",
    "OUTPUT_PATH = Path(\"experiment_results\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PLOT_RESULTS = True\n",
    "PLOT_RESULTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add here if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, I loaded the data as `XournalppDocument` but that approach lacked ground truth data. Instead, I now load the annotated data, which comes with ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_bboxes = load_list_of_bboxes(\n",
    "    \"../tests/data/2024-10-13_minimal.annotations.json\"\n",
    ")\n",
    "\n",
    "DPI = 72  # TODO: Add this to annotations!\n",
    "\n",
    "# TODO: Maybe integrate `/DPI` into the x and y values? Maybe convert to cm?\n",
    "# TODO: Add page dimensions, i.e.:\n",
    "# - float(page.meta_data[\"width\"]) / DPI,\n",
    "# - float(page.meta_data[\"height\"]) / DPI,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========\n",
    "# Figure 1\n",
    "# ========\n",
    "\n",
    "length = len(annotated_bboxes[\"bboxes\"])\n",
    "nr_2 = 4\n",
    "nr_1 = length // nr_2 + 1\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nr_1, ncols=nr_2, figsize=(10, 8))\n",
    "\n",
    "for i_bbox in range(length):\n",
    "    bbox = annotated_bboxes[\"bboxes\"][i_bbox]\n",
    "\n",
    "    a = axes.flatten()[i_bbox]\n",
    "\n",
    "    a.set_aspect(\"equal\")\n",
    "    a.set_title(bbox[\"text\"])\n",
    "    a.set_xlabel(\"x\")\n",
    "    a.set_ylabel(\"-y\")\n",
    "\n",
    "    for bbox_stroke in bbox[\"bbox_strokes\"]:\n",
    "        x = bbox_stroke[\"x\"] / DPI\n",
    "        y = bbox_stroke[\"y\"] / DPI\n",
    "        a.scatter(x, -y, c=\"black\", s=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========\n",
    "# Figure 2\n",
    "# ========\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "a = plt.gca()\n",
    "a.set_aspect(\"equal\")\n",
    "a.set_xlabel(\"x\")\n",
    "a.set_ylabel(\"-y\")\n",
    "\n",
    "for i_bbox in range(length):\n",
    "    bbox = annotated_bboxes[\"bboxes\"][i_bbox]\n",
    "\n",
    "    # Draw bbox\n",
    "    xy = (\n",
    "        min([bbox[\"point_1_x\"], bbox[\"point_2_x\"]]) / DPI,\n",
    "        min([-bbox[\"point_1_y\"], -bbox[\"point_2_y\"]])\n",
    "        / DPI,  # TODO: This messing around w/ y coord sign is annoying\n",
    "    )\n",
    "    dx = np.abs(bbox[\"point_1_x\"] - bbox[\"point_2_x\"]) / DPI\n",
    "    dy = np.abs(bbox[\"point_1_y\"] - bbox[\"point_2_y\"]) / DPI\n",
    "    a.add_patch(\n",
    "        patches.Rectangle(xy, dx, dy, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "    )\n",
    "\n",
    "    # Draw label\n",
    "    a.text(x=xy[0], y=xy[1] + dy, s=bbox[\"text\"], c=\"red\")\n",
    "\n",
    "    for bbox_stroke in bbox[\"bbox_strokes\"]:\n",
    "        x = bbox_stroke[\"x\"] / DPI\n",
    "        y = bbox_stroke[\"y\"] / DPI\n",
    "        a.scatter(x, -y, c=\"black\", s=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare list of all strokes w/ relevant meta information as ground truth. This variable serves as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strokes_data = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "    \"x_mean\": [],\n",
    "    \"y_mean\": [],\n",
    "    \"i_bbox\": [],\n",
    "    \"text\": [],\n",
    "}\n",
    "\n",
    "for i_bbox in range(len(annotated_bboxes[\"bboxes\"])):\n",
    "    bbox = annotated_bboxes[\"bboxes\"][i_bbox]\n",
    "\n",
    "    for bbox_stroke in bbox[\"bbox_strokes\"]:\n",
    "        x = +bbox_stroke[\"x\"] / DPI\n",
    "        y = -bbox_stroke[\"y\"] / DPI\n",
    "\n",
    "        df_strokes_data[\"x\"].append(x)\n",
    "        df_strokes_data[\"y\"].append(y)\n",
    "        df_strokes_data[\"x_mean\"].append(np.mean(x))\n",
    "        df_strokes_data[\"y_mean\"].append(np.mean(y))\n",
    "        df_strokes_data[\"i_bbox\"].append(i_bbox)\n",
    "        df_strokes_data[\"text\"].append(bbox[\"text\"])\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(df_strokes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "a = plt.gca()\n",
    "a.set_aspect(\"equal\")\n",
    "a.set_xlabel(\"x\")\n",
    "a.set_ylabel(\"y\")\n",
    "\n",
    "for (i_bbox, text), df_grouped in df_train.groupby(\n",
    "    [\"i_bbox\", \"text\"],\n",
    "):\n",
    "    a.scatter(df_grouped[\"x_mean\"], df_grouped[\"y_mean\"], c=\"red\", s=2, zorder=999)\n",
    "\n",
    "    bottom_left_x = np.inf\n",
    "    bottom_left_y = np.inf\n",
    "    top_right_x = -np.inf\n",
    "    top_right_y = -np.inf\n",
    "    for _, row in df_grouped.iterrows():\n",
    "        a.plot(row.x, row.y)  # , c=cmap(i_row/N))\n",
    "        if row.x.min() < bottom_left_x:\n",
    "            bottom_left_x = row.x.min()\n",
    "        if row.y.min() < bottom_left_y:\n",
    "            bottom_left_y = row.y.min()\n",
    "        if row.x.max() > top_right_x:\n",
    "            top_right_x = row.x.max()\n",
    "        if row.y.max() > top_right_y:\n",
    "            top_right_y = row.y.max()\n",
    "\n",
    "    # Plot bounding box\n",
    "    xy = (bottom_left_x, bottom_left_y)\n",
    "    dx = top_right_x - bottom_left_x\n",
    "    dy = top_right_y - bottom_left_y\n",
    "    a.add_patch(\n",
    "        patches.Rectangle(xy, dx, dy, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "    )\n",
    "\n",
    "    # Plot text\n",
    "    a.text(x=bottom_left_x, y=top_right_y, s=f'\"{text}\" ({i_bbox})', c=\"red\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_clusterings = [\n",
    "    AgglomerativeClustering(\n",
    "        n_clusters=22, distance_threshold=None\n",
    "    ),  # I hard-code 22 b/c I counted that there're 22 clusters\n",
    "    AgglomerativeClustering(n_clusters=10, distance_threshold=None),\n",
    "    AgglomerativeClustering(\n",
    "        n_clusters=None, distance_threshold=1e0\n",
    "    ),  # One could maybe tune it by investigating nr of clusters over distance threshold; TODO: Distance threshold using distribution?!\n",
    "    SpectralClustering(\n",
    "        n_clusters=15,  # 21,\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    ),\n",
    "    SpectralClustering(\n",
    "        n_clusters=21,  # 21,\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    ),\n",
    "    SpectralClustering(\n",
    "        n_clusters=6,  # 21,\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    ),\n",
    "    MeanShift(\n",
    "        bandwidth=None,\n",
    "    ),\n",
    "    MeanShift(\n",
    "        bandwidth=0.1,\n",
    "    ),\n",
    "    MeanShift(\n",
    "        bandwidth=1.0,\n",
    "    ),\n",
    "    MeanShift(\n",
    "        bandwidth=10.0,\n",
    "    ),\n",
    "    AffinityPropagation(),\n",
    "    HDBSCAN(min_cluster_size=2),\n",
    "    # FeatureAgglomeration(\n",
    "    #     n_clusters=None,\n",
    "    #     distance_threshold=0.1,\n",
    "    # ),\n",
    "    # FeatureAgglomeration(\n",
    "    #     n_clusters=None,\n",
    "    #     distance_threshold=1.0,\n",
    "    # ),\n",
    "    # FeatureAgglomeration(\n",
    "    #     n_clusters=None,\n",
    "    #     distance_threshold=10.0,\n",
    "    # ),\n",
    "]\n",
    "\n",
    "all_clusterings += [DBSCAN(eps) for eps in np.logspace(-4, 1, 1000)]\n",
    "all_clusterings += [\n",
    "    AgglomerativeClustering(n_clusters=None, distance_threshold=DISTANCE_THRESHOLD)\n",
    "    for DISTANCE_THRESHOLD in np.logspace(-4, 1, 1000)\n",
    "]\n",
    "\n",
    "results = {\n",
    "    \"index\": [],\n",
    "    \"score\": [],\n",
    "}\n",
    "for i_clustering, clustering in enumerate(all_clusterings):\n",
    "    print(i_clustering, clustering)\n",
    "    clustering.fit(df_train[[\"x_mean\", \"y_mean\"]])\n",
    "\n",
    "    score = adjusted_rand_score(df_train[\"i_bbox\"], clustering.labels_)\n",
    "\n",
    "    results[\"index\"].append(i_clustering)\n",
    "    results[\"score\"].append(score)\n",
    "\n",
    "    # Plotting\n",
    "    if PLOT_RESULTS:\n",
    "        fig, [a_ground_truth, a_predicted] = plt.subplots(1, 2, figsize=(10, 8))\n",
    "        plot_clustered_document(\n",
    "            a_ground_truth,\n",
    "            a_predicted,\n",
    "            clustering,\n",
    "            annotated_bboxes,\n",
    "            DPI,\n",
    "            df_train,\n",
    "            a_predicted_title=f\"A-RAND={score}\",\n",
    "        )\n",
    "        plt.savefig(OUTPUT_PATH / f\"iClustering{i_clustering}.png\")\n",
    "        plt.close()\n",
    "\n",
    "results = pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(results[\"index\"], results[\"score\"], c=\"red\")\n",
    "\n",
    "plt.xlabel(\"Index of clustering settings\")\n",
    "plt.ylabel(\"Adjusted Rand Score (larger is better)\")\n",
    "plt.savefig(\"2024-10-18_clustering_experiments.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check if the clusters make sense by plotting the clusters on the page of a set of pre-selected settings to test out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO!!!\n",
    "\n",
    "# CONTINUE TO WORK HERE!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Learning: The peak at ~800 seems to classify rows of text. This should be fine w/ OnlineHTR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more stroke features. Then run large screen. Also add feature selection.\n",
    "# TODO: Maybe add k fold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the dendrogram, see [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try out DBSCAN! Also see [here](https://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also next, try out another document to play around with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Is my OnlineHTR model robust against rotated text?! Maybe one should rotate the text first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is probably worth it to write a bit of infrastructure code to experiment more (and easier and easier to compare) with these clustering approaches.\n",
    "\n",
    "Next: Feed these sequences to `OnlineHTR` or retrained `SimpleHTR` nmodel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs:\n",
    "\n",
    "- I think next cool thing to try out is to do proper feature engineering to try to enhance the features. Using the raw strokes could be regarded as last resort but IMHO doesn't make sense b/c a stroke always has a single word attached as strokes cannot be split, which they could be if one allows clusterings on the raw datapoints instead of strokes.\n",
    "\n",
    "- Good source for rand score: [see here](https://stats.stackexchange.com/questions/260229/comparing-a-clustering-algorithm-partition-to-a-ground-truth-one).\n",
    "- After finding the best clustering, do apply OnlineHTR to check how it performs!\n",
    "- To overcome the scale issue (i.e. everyone's handwriting scale is a wee bit different), one would need to use an approach that is based on 'nearest neighbours'. This works b/c one does not write on top of existing words.\n",
    "    - also, one could weight the x direction more in definition of closeness/distance\n",
    "- Hook up OnlineHTR to here!\n",
    "- I think the biggest problem for the OnlineHTR model would be the different line positions based on the way it was trained. Hence, one could maybe put extra emphasis on clusters being on similar y values.\n",
    "- I have to say that I am unclear if a heuristic (i.e. a clustering algo w/ smartly chosen parameters) is really enough. Certainly for now, but a fully data-driven way would be better to accommodate different writers. This is probably relevant for a next iteration of the model.\n",
    "    - E.g., is this approach robust against larger handwriting?\n",
    "- Hyper parameters like distance threshold are probably a function of the content of the page (e.g. diagrams, written text height, etc).\n",
    "- It would be cool to try graph NN. Also, I'd love to add more features than the mean. That might help in learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xournalpp_htr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
